{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bb5018f-c576-42f3-a2a5-d224db4760f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Constants\n",
    "num_slides = 250\n",
    "num_patches_per_slide = 250\n",
    "patch_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a0c8162-f059-4eaa-a212-97401346cb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_patches_dir_brca = \"/lotterlab/users/vmishra/RSA_updated100/preprocessed_patches_BRCA\"\n",
    "preprocessed_patches_dir_luad = \"/lotterlab/users/vmishra/RSA_updated100/preprocessed_patches_LUAD\"\n",
    "preprocessed_patches_dir_lusc = \"/lotterlab/users/vmishra/RSA_updated100/preprocessed_patches_LUSC\"\n",
    "preprocessed_patches_dir_coad = \"/lotterlab/users/vmishra/RSA_updated100/preprocessed_patches_COAD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38f882bf-1fd1-42df-8fa8-7608c0231ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(patches, model, preprocess, device, batch_size=64, verbose=True):\n",
    "    num_batches = ceil(len(patches) / batch_size)\n",
    "    opt_embs = []\n",
    "\n",
    "    for batch_idx in tqdm(range(num_batches), disable=not verbose):\n",
    "        start = batch_idx * batch_size\n",
    "        end = min(start + batch_size, len(patches))\n",
    "        batch_np = patches[start:end]\n",
    "\n",
    "        try:\n",
    "            # Convert numpy arrays to PIL Images\n",
    "            batch_pil = [Image.fromarray(patch.astype('uint8')) for patch in batch_np]\n",
    "            \n",
    "            # Apply CONCH preprocessing to each image individually\n",
    "            batch_transformed = torch.stack([preprocess(img) for img in batch_pil])\n",
    "            \n",
    "            # Move batch to device\n",
    "            batch = batch_transformed.to(device)\n",
    "\n",
    "            # Get embeddings\n",
    "            with torch.no_grad():\n",
    "                batch_emb = model.encode_image(batch)\n",
    "\n",
    "            # Move to CPU and append\n",
    "            opt_embs.append(batch_emb.cpu())\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {batch_idx}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    if not opt_embs:\n",
    "        return np.array([])\n",
    "\n",
    "    # Stack all embeddings\n",
    "    opt_embs = torch.cat(opt_embs, dim=0)\n",
    "    \n",
    "    return opt_embs.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ba2c62b-d46e-4ed2-8212-5d623e242a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_patches_from_individual_files(patches_dir, normalized=False):\n",
    "    patches_list = []\n",
    "    \n",
    "    if not os.path.exists(patches_dir):\n",
    "        print(f\"Directory not found: {patches_dir}\")\n",
    "        return np.array([])\n",
    "    \n",
    "    if normalized:\n",
    "        pattern = \"_patches-normalized.npy\"\n",
    "    else:\n",
    "        pattern = \"_patches.npy\"\n",
    "    \n",
    "    filenames = [f for f in os.listdir(patches_dir) if f.endswith(pattern)]\n",
    "    \n",
    "    if not filenames:\n",
    "        print(f\"No files found matching pattern '{pattern}' in {patches_dir}\")\n",
    "        return np.array([])\n",
    "    \n",
    "    print(f\"Found {len(filenames)} patch files in {patches_dir}\")\n",
    "    \n",
    "    for filename in tqdm(filenames, desc=f\"Loading {'normalized' if normalized else 'original'} patches\"):\n",
    "        try:\n",
    "            patches = np.load(os.path.join(patches_dir, filename))\n",
    "            patches_list.append(patches)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if patches_list:\n",
    "        all_patches = np.concatenate(patches_list, axis=0)\n",
    "        print(f\"Total patches loaded: {len(all_patches)}\")\n",
    "        return all_patches\n",
    "    else:\n",
    "        print(\"No patches could be loaded\")\n",
    "        return np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12c90313-c23c-4ab6-9816-caf77f450fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_patches(patches, model, preprocess, device):\n",
    "    if len(patches) == 0:\n",
    "        return np.array([])\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return embed(patches, model, preprocess, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c37c08-b0ad-4fc6-86f5-74e3171b02a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/Mahmoodlab/CONCH.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e028e9-6356-45b2-b0c4-60ba5d5a6b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this: !rm conch.py if not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d3fb312-7699-40a5-a494-3182df3ed254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes2/vmishra/miniconda3/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from conch.open_clip_custom import create_model_from_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "443672b9-630d-4c17-a5b1-1a1df89c08c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CoCa(\n",
       "  (text): TextTransformer(\n",
       "    (token_embedding): Embedding(32007, 768)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-11): 12 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual): VisualModel(\n",
       "    (trunk): VisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (patch_drop): Identity()\n",
       "      (norm_pre): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (6): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (7): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (8): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (9): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (10): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (11): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (fc_norm): Identity()\n",
       "      (head_drop): Dropout(p=0.0, inplace=False)\n",
       "      (head): Identity()\n",
       "    )\n",
       "    (attn_pool_contrast): AttentionalPooler(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_k): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (ln_contrast): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (head): Sequential()\n",
       "    (attn_pool_caption): AttentionalPooler(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_q): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_k): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (ln_caption): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_decoder): MultimodalTransformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (cross_attn): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_1_kv): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, preprocess = create_model_from_pretrained(\n",
    "    'conch_ViT-B-16',\n",
    "    \"hf_hub:MahmoodLab/conch\",\n",
    "    hf_auth_token=\"YOUR_HF_TOKEN\"\n",
    ")\n",
    "model.eval()\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cdee77c-406e-456c-8e5d-477ba42a1ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 patch files in /lotterlab/users/vmishra/RSA_updated100/preprocessed_patches_BRCA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading original patches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 145.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patches loaded: 750\n",
      "Found 3 patch files in /lotterlab/users/vmishra/RSA_updated100/preprocessed_patches_BRCA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading normalized patches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 137.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patches loaded: 750\n",
      "Found 3 patch files in /lotterlab/users/vmishra/RSA_updated100/preprocessed_patches_LUAD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading original patches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 146.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patches loaded: 750\n",
      "Found 3 patch files in /lotterlab/users/vmishra/RSA_updated100/preprocessed_patches_LUAD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading normalized patches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 149.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patches loaded: 750\n",
      "Found 3 patch files in /lotterlab/users/vmishra/RSA_updated100/preprocessed_patches_LUSC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading original patches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 149.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patches loaded: 750\n",
      "Found 3 patch files in /lotterlab/users/vmishra/RSA_updated100/preprocessed_patches_LUSC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading normalized patches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 152.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patches loaded: 750\n",
      "Found 3 patch files in /lotterlab/users/vmishra/RSA_updated100/preprocessed_patches_COAD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading original patches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 151.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patches loaded: 750\n",
      "Found 3 patch files in /lotterlab/users/vmishra/RSA_updated100/preprocessed_patches_COAD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading normalized patches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 152.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patches loaded: 750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:06<00:00,  1.80it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:06<00:00,  1.87it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:06<00:00,  1.86it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:06<00:00,  1.74it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:06<00:00,  1.84it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:06<00:00,  1.81it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:06<00:00,  1.83it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:07<00:00,  1.71it/s]\n"
     ]
    }
   ],
   "source": [
    "brca_patches = load_patches_from_individual_files(preprocessed_patches_dir_brca, normalized=False)\n",
    "brca_patches_norm = load_patches_from_individual_files(preprocessed_patches_dir_brca, normalized=True)\n",
    "\n",
    "luad_patches = load_patches_from_individual_files(preprocessed_patches_dir_luad, normalized=False)\n",
    "luad_patches_norm = load_patches_from_individual_files(preprocessed_patches_dir_luad, normalized=True)\n",
    "\n",
    "lusc_patches = load_patches_from_individual_files(preprocessed_patches_dir_lusc, normalized=False)\n",
    "lusc_patches_norm = load_patches_from_individual_files(preprocessed_patches_dir_lusc, normalized=True)\n",
    "\n",
    "coad_patches = load_patches_from_individual_files(preprocessed_patches_dir_coad, normalized=False)\n",
    "coad_patches_norm = load_patches_from_individual_files(preprocessed_patches_dir_coad, normalized=True)\n",
    "\n",
    "brca_embeddings = embed_patches(brca_patches, model, preprocess, device)\n",
    "luad_embeddings = embed_patches(luad_patches, model, preprocess, device)\n",
    "lusc_embeddings = embed_patches(lusc_patches, model, preprocess, device)\n",
    "coad_embeddings = embed_patches(coad_patches, model, preprocess, device)\n",
    "\n",
    "brca_embeddings_norm = embed_patches(brca_patches_norm, model, preprocess, device)\n",
    "luad_embeddings_norm = embed_patches(luad_patches_norm, model, preprocess, device)\n",
    "lusc_embeddings_norm = embed_patches(lusc_patches_norm, model, preprocess, device)\n",
    "coad_embeddings_norm = embed_patches(coad_patches_norm, model, preprocess, device)\n",
    "\n",
    "num_brca = len(brca_embeddings)\n",
    "num_luad = len(luad_embeddings)\n",
    "num_lusc = len(lusc_embeddings)\n",
    "num_coad = len(coad_embeddings)\n",
    "\n",
    "num_brca_norm = len(brca_embeddings_norm)\n",
    "num_luad_norm = len(luad_embeddings_norm)\n",
    "num_lusc_norm = len(lusc_embeddings_norm)\n",
    "num_coad_norm = len(coad_embeddings_norm)\n",
    "\n",
    "brca_labels = [f\"BRCA_{i+1}\" for i in range(num_brca)]\n",
    "luad_labels = [f\"LUAD_{i+1}\" for i in range(num_luad)]\n",
    "lusc_labels = [f\"LUSC_{i+1}\" for i in range(num_lusc)]\n",
    "coad_labels = [f\"COAD_{i+1}\" for i in range(num_coad)]\n",
    "\n",
    "brca_labels_norm = [f\"BRCA_norm_{i+1}\" for i in range(num_brca_norm)]\n",
    "luad_labels_norm = [f\"LUAD_norm_{i+1}\" for i in range(num_luad_norm)]\n",
    "lusc_labels_norm = [f\"LUSC_norm_{i+1}\" for i in range(num_lusc_norm)]\n",
    "coad_labels_norm = [f\"COAD_norm_{i+1}\" for i in range(num_coad_norm)]\n",
    "\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/brca_embeddings_conch_updated.npy\", brca_embeddings)\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/luad_embeddings_conch_updated.npy\", luad_embeddings)\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/lusc_embeddings_conch_updated.npy\", lusc_embeddings)\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/coad_embeddings_conch_updated.npy\", coad_embeddings)\n",
    "\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/brca_embeddings_conch_normalized_updated.npy\", brca_embeddings_norm)\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/luad_embeddings_conch_normalized_updated.npy\", luad_embeddings_norm)\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/lusc_embeddings_conch_normalized_updated.npy\", lusc_embeddings_norm)\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/coad_embeddings_conch_normalized_updated.npy\", coad_embeddings_norm)\n",
    "\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/brca_labels_conch_updated.npy\", brca_labels)\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/luad_labels_conch_updated.npy\", luad_labels)\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/lusc_labels_conch_updated.npy\", lusc_labels)\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/coad_labels_conch_updated.npy\", coad_labels)\n",
    "\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/brca_labels_conch_norm_updated.npy\", brca_labels_norm)\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/luad_labels_conch_norm_updated.npy\", luad_labels_norm)\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/lusc_labels_conch_norm_updated.npy\", lusc_labels_norm)\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/coad_labels_conch_norm_updated.npy\", coad_labels_norm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
