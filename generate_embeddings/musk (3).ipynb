{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e692d371-b357-4714-917a-06fd4887e35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/lilab-stanford/MUSK.git\n",
      "  Cloning https://github.com/lilab-stanford/MUSK.git to /tmp/pip-req-build-42era1aw\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/lilab-stanford/MUSK.git /tmp/pip-req-build-42era1aw\n",
      "  Resolved https://github.com/lilab-stanford/MUSK.git to commit fc9421aaebb2a3651fed5b69558c306f2836c228\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: musk\n",
      "  Building wheel for musk (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for musk: filename=musk-1.0.0-py3-none-any.whl size=51766 sha256=2a44c5481ac837b58e4bb3cd9b4efbf860f88ff2996d6d31310c558faaf92d99\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-qldhnzd_/wheels/05/9d/56/a69e763dd2663e34d1a36ceb4feec33f79f036d7cd20fa7396\n",
      "Successfully built musk\n",
      "Installing collected packages: musk\n",
      "Successfully installed musk-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/lilab-stanford/MUSK.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39b45459-97ef-49dc-80c4-e07af2a8652c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fairscale\n",
      "  Using cached fairscale-0.4.13.tar.gz (266 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.8.0 in ./miniconda3/lib/python3.12/site-packages (from fairscale) (2.5.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in ./miniconda3/lib/python3.12/site-packages (from fairscale) (1.26.4)\n",
      "Requirement already satisfied: filelock in ./miniconda3/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./miniconda3/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (4.14.1)\n",
      "Requirement already satisfied: networkx in ./miniconda3/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./miniconda3/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./miniconda3/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./miniconda3/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./miniconda3/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./miniconda3/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./miniconda3/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./miniconda3/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./miniconda3/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./miniconda3/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./miniconda3/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./miniconda3/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./miniconda3/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./miniconda3/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./miniconda3/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./miniconda3/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (3.1.0)\n",
      "Requirement already satisfied: setuptools in ./miniconda3/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./miniconda3/lib/python3.12/site-packages (from torch>=1.8.0->fairscale) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.8.0->fairscale) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./miniconda3/lib/python3.12/site-packages (from jinja2->torch>=1.8.0->fairscale) (3.0.2)\n",
      "Building wheels for collected packages: fairscale\n",
      "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332208 sha256=662df6dc9399d1675a801cebc8b795c96289ae9c629bc3de9d26c31f35951fc5\n",
      "  Stored in directory: /homes2/vmishra/.cache/pip/wheels/5a/88/aa/d84b2cf1bad6b273cbf661640141a82c7b9f496e024f80aac0\n",
      "Successfully built fairscale\n",
      "Installing collected packages: fairscale\n",
      "Successfully installed fairscale-0.4.13\n"
     ]
    }
   ],
   "source": [
    "!pip install fairscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bb5018f-c576-42f3-a2a5-d224db4760f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login\n",
    "import torchvision\n",
    "from timm.models import create_model\n",
    "from timm.data.constants import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n",
    "from musk import utils\n",
    "from musk import modeling\n",
    "\n",
    "# Constants\n",
    "num_slides = 250\n",
    "num_patches_per_slide = 250\n",
    "patch_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a0c8162-f059-4eaa-a212-97401346cb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_patches_dir_brca = \"/lotterlab/users/vmishra/RSA_updated100/preprocessed_patches_BRCA\"\n",
    "preprocessed_patches_dir_luad = \"/lotterlab/users/vmishra/RSA_updated100/preprocessed_patches_LUAD\"\n",
    "preprocessed_patches_dir_lusc = \"/lotterlab/users/vmishra/RSA_updated100/preprocessed_patches_LUSC\"\n",
    "preprocessed_patches_dir_coad = \"/lotterlab/users/vmishra/RSA_updated100/preprocessed_patches_COAD\"\n",
    "\n",
    "login(token = \"YOUR_HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38f882bf-1fd1-42df-8fa8-7608c0231ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(\n",
    "    patches,\n",
    "    model,\n",
    "    transform,\n",
    "    device,\n",
    "    batch_size=64,\n",
    "    verbose=True,\n",
    "):\n",
    "    num_batches = ceil(len(patches) / batch_size)\n",
    "    opt_embs = []\n",
    "\n",
    "    for batch_idx in tqdm(range(num_batches), disable=not verbose):\n",
    "        # Slice batch\n",
    "        start = batch_idx * batch_size\n",
    "        end = min(start + batch_size, len(patches))\n",
    "        batch_np = patches[start:end]\n",
    "\n",
    "        # Convert numpy arrays to PIL Images for transform\n",
    "        batch_pil = [Image.fromarray(patch.astype('uint8')).convert(\"RGB\") for patch in batch_np]\n",
    "        \n",
    "        # Apply transform to each image\n",
    "        batch_transformed = [transform(img) for img in batch_pil]\n",
    "        \n",
    "        # Stack transformed images\n",
    "        batch = torch.stack(batch_transformed).to(device, dtype=torch.float16)\n",
    "\n",
    "        # Call MUSK model\n",
    "        with torch.inference_mode():\n",
    "            batch_emb = model(\n",
    "                image=batch,\n",
    "                with_head=False,\n",
    "                out_norm=False,\n",
    "                ms_aug=True,\n",
    "                return_global=True  \n",
    "            )[0]\n",
    "\n",
    "        # Copy to host and append\n",
    "        opt_embs.append(batch_emb.cpu())\n",
    "\n",
    "    # Stack to contiguous array\n",
    "    opt_embs = torch.cat(opt_embs, dim=0)\n",
    "\n",
    "    return opt_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ba2c62b-d46e-4ed2-8212-5d623e242a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_patches_from_individual_files(patches_dir, normalized=False):\n",
    "    patches_list = []\n",
    "    \n",
    "    if not os.path.exists(patches_dir):\n",
    "        print(f\"Directory not found: {patches_dir}\")\n",
    "        return np.array([])\n",
    "    \n",
    "    if normalized:\n",
    "        pattern = \"_patches-normalized.npy\"\n",
    "    else:\n",
    "        pattern = \"_patches.npy\"\n",
    "    \n",
    "    filenames = [f for f in os.listdir(patches_dir) if f.endswith(pattern)]\n",
    "    \n",
    "    if not filenames:\n",
    "        print(f\"No files found matching pattern '{pattern}' in {patches_dir}\")\n",
    "        return np.array([])\n",
    "    \n",
    "    print(f\"Found {len(filenames)} patch files in {patches_dir}\")\n",
    "    \n",
    "    for filename in tqdm(filenames, desc=f\"Loading {'normalized' if normalized else 'original'} patches\"):\n",
    "        try:\n",
    "            patches = np.load(os.path.join(patches_dir, filename))\n",
    "            patches_list.append(patches)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if patches_list:\n",
    "        all_patches = np.concatenate(patches_list, axis=0)\n",
    "        print(f\"Total patches loaded: {len(all_patches)}\")\n",
    "        return all_patches\n",
    "    else:\n",
    "        print(\"No patches could be loaded\")\n",
    "        return np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12c90313-c23c-4ab6-9816-caf77f450fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_patches(patches, model, transform, device):\n",
    "    if len(patches) == 0:\n",
    "        return np.array([])\n",
    "    \n",
    "    return embed(patches, model, transform, device).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "443672b9-630d-4c17-a5b1-1a1df89c08c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b1f929b8f7437b91f693d84d8ef92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.35G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load ckpt from hf_hub:xiangjx/musk\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = create_model(\"musk_large_patch16_384\")\n",
    "utils.load_model_and_may_interpolate(\"hf_hub:xiangjx/musk\", model, 'model|module', '')\n",
    "model.to(device=device, dtype=torch.float16)\n",
    "model.eval()\n",
    "\n",
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(384, interpolation=3, antialias=True),\n",
    "    torchvision.transforms.CenterCrop((384, 384)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2cdee77c-406e-456c-8e5d-477ba42a1ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 patch files in /lotterlab/users/vmishra/RSA_updated100/preprocessed_patches_BRCA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading original patches: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 58.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patches loaded: 750\n",
      "Found 3 patch files in /lotterlab/users/vmishra/RSA_updated100/preprocessed_patches_BRCA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading normalized patches: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 71.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patches loaded: 750\n",
      "Found 3 patch files in /lotterlab/users/vmishra/RSA_updated100/preprocessed_patches_LUAD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading original patches: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 66.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patches loaded: 750\n",
      "Found 3 patch files in /lotterlab/users/vmishra/RSA_updated100/preprocessed_patches_LUAD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading normalized patches: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 66.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patches loaded: 750\n",
      "Found 3 patch files in /lotterlab/users/vmishra/RSA_updated100/preprocessed_patches_LUSC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading original patches: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 67.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patches loaded: 750\n",
      "Found 3 patch files in /lotterlab/users/vmishra/RSA_updated100/preprocessed_patches_LUSC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading normalized patches: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 66.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patches loaded: 750\n",
      "Found 3 patch files in /lotterlab/users/vmishra/RSA_updated100/preprocessed_patches_COAD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading original patches: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 70.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patches loaded: 750\n",
      "Found 3 patch files in /lotterlab/users/vmishra/RSA_updated100/preprocessed_patches_COAD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading normalized patches: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 72.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patches loaded: 750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:20<00:00,  1.68s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:17<00:00,  1.49s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:17<00:00,  1.48s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:17<00:00,  1.48s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:17<00:00,  1.48s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:17<00:00,  1.49s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:17<00:00,  1.49s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:17<00:00,  1.48s/it]\n"
     ]
    }
   ],
   "source": [
    "brca_patches = load_patches_from_individual_files(preprocessed_patches_dir_brca, normalized=False)\n",
    "brca_patches_norm = load_patches_from_individual_files(preprocessed_patches_dir_brca, normalized=True)\n",
    "\n",
    "luad_patches = load_patches_from_individual_files(preprocessed_patches_dir_luad, normalized=False)\n",
    "luad_patches_norm = load_patches_from_individual_files(preprocessed_patches_dir_luad, normalized=True)\n",
    "\n",
    "lusc_patches = load_patches_from_individual_files(preprocessed_patches_dir_lusc, normalized=False)\n",
    "lusc_patches_norm = load_patches_from_individual_files(preprocessed_patches_dir_lusc, normalized=True)\n",
    "\n",
    "coad_patches = load_patches_from_individual_files(preprocessed_patches_dir_coad, normalized=False)\n",
    "coad_patches_norm = load_patches_from_individual_files(preprocessed_patches_dir_coad, normalized=True)\n",
    "\n",
    "brca_embeddings = embed_patches(brca_patches, model, preprocess, device)\n",
    "luad_embeddings = embed_patches(luad_patches, model, preprocess, device)\n",
    "lusc_embeddings = embed_patches(lusc_patches, model, preprocess, device)\n",
    "coad_embeddings = embed_patches(coad_patches, model, preprocess, device)\n",
    "\n",
    "brca_embeddings_norm = embed_patches(brca_patches_norm, model, preprocess, device)\n",
    "luad_embeddings_norm = embed_patches(luad_patches_norm, model, preprocess, device)\n",
    "lusc_embeddings_norm = embed_patches(lusc_patches_norm, model, preprocess, device)\n",
    "coad_embeddings_norm = embed_patches(coad_patches_norm, model, preprocess, device)\n",
    "\n",
    "num_brca = len(brca_embeddings)\n",
    "num_luad = len(luad_embeddings)\n",
    "num_lusc = len(lusc_embeddings)\n",
    "num_coad = len(coad_embeddings)\n",
    "\n",
    "num_brca_norm = len(brca_embeddings_norm)\n",
    "num_luad_norm = len(luad_embeddings_norm)\n",
    "num_lusc_norm = len(lusc_embeddings_norm)\n",
    "num_coad_norm = len(coad_embeddings_norm)\n",
    "\n",
    "brca_labels = [f\"BRCA_{i+1}\" for i in range(num_brca)]\n",
    "luad_labels = [f\"LUAD_{i+1}\" for i in range(num_luad)]\n",
    "lusc_labels = [f\"LUSC_{i+1}\" for i in range(num_lusc)]\n",
    "coad_labels = [f\"COAD_{i+1}\" for i in range(num_coad)]\n",
    "\n",
    "brca_labels_norm = [f\"BRCA_norm_{i+1}\" for i in range(num_brca_norm)]\n",
    "luad_labels_norm = [f\"LUAD_norm_{i+1}\" for i in range(num_luad_norm)]\n",
    "lusc_labels_norm = [f\"LUSC_norm_{i+1}\" for i in range(num_lusc_norm)]\n",
    "coad_labels_norm = [f\"COAD_norm_{i+1}\" for i in range(num_coad_norm)]\n",
    "\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/brca_embeddings_musk_updated.npy\", brca_embeddings)\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/luad_embeddings_musk_updated.npy\", luad_embeddings)\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/lusc_embeddings_musk_updated.npy\", lusc_embeddings)\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/coad_embeddings_musk_updated.npy\", coad_embeddings)\n",
    "\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/brca_embeddings_musk_normalized_updated.npy\", brca_embeddings_norm)\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/luad_embeddings_musk_normalized_updated.npy\", luad_embeddings_norm)\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/lusc_embeddings_musk_normalized_updated.npy\", lusc_embeddings_norm)\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/coad_embeddings_musk_normalized_updated.npy\", coad_embeddings_norm)\n",
    "\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/brca_labels_musk_updated.npy\", brca_labels)\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/luad_labels_musk_updated.npy\", luad_labels)\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/lusc_labels_musk_updated.npy\", lusc_labels)\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/coad_labels_musk_updated.npy\", coad_labels)\n",
    "\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/brca_labels_musk_norm_updated.npy\", brca_labels_norm)\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/luad_labels_musk_norm_updated.npy\", luad_labels_norm)\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/lusc_labels_musk_norm_updated.npy\", lusc_labels_norm)\n",
    "np.save(\"/lotterlab/users/vmishra/RSA_updated100/coad_labels_musk_norm_updated.npy\", coad_labels_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73726afe-8bea-41f0-a131-a80d34ef3a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
